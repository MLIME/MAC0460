{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch básico 1: primeiras noções\n",
    "\n",
    "Nesse notebook vamos usar uma biblioteca para deep learning\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td>\n",
    "<img align=\"middle\"  width='400' heith='100'  src='images/pytorch-logo-dark.png'>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "Você deve ir no [site oficial](https://pytorch.org/) para instalar essa bibloteca no seu computador. \n",
    "\n",
    "\n",
    "Vamos usar essa biblioteca pois\n",
    "\n",
    "- Com ela podemos fazer uso de GPU (importante quando treinamos grandes datasets).\n",
    "\n",
    "- Ela possui uma série de facilidades para se montar modelos de deep learning (como diferenciação automática e certas funções já pré definidas).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notebook feito para a versão 0.4.0 \n",
    "import numpy as np\n",
    "import torch\n",
    "print(\"PyTorch version = {} \".format(torch.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hello, PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Tensor](https://pytorch.org/docs/master/tensors.html)\n",
    "\n",
    "O objeto central com que vamos trabalhar é um **Tensor**. \n",
    "\n",
    "Um tensor nada mais é do que uma matriz multidimensional (como o ndarray do NumPy) com elementos de um único tipo. Tensores podem ser usados em GPU para acelerar o processo de computação. Para os curiosos, nesse [blog post](http://blog.christianperone.com/2018/03/pytorch-internal-architecture-tour/) há uma descrição sobre o funcionamento interno dos tensores do PyTorch implementados em C/C++.\n",
    "\n",
    "Em diferentes aplicações temos que ter bem claro o tipo dos tensores que estamos usando, por exemplo em imagens usamos um torch.ByteTensor para economizar memória. O tipo padrão é torch.FloatTensor. Vale a pena saber quais tipos estão disponíveis nessa biblioteca.\n",
    "\n",
    "<table border=\"1\" class=\"docutils\">\n",
    "<colgroup>\n",
    "<col width=\"19%\">\n",
    "<col width=\"34%\">\n",
    "<col width=\"21%\">\n",
    "<col width=\"25%\">\n",
    "</colgroup>\n",
    "<thead valign=\"bottom\">\n",
    "<tr class=\"row-odd\"><th class=\"head\">Data type</th>\n",
    "<th class=\"head\">dtype</th>\n",
    "<th class=\"head\">CPU tensor</th>\n",
    "<th class=\"head\">GPU tensor</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody valign=\"top\">\n",
    "<tr class=\"row-even\"><td>32-bit floating point</td>\n",
    "<td><code class=\"docutils literal notranslate\"><span class=\"pre\">torch.float32</span></code> or <code class=\"docutils literal notranslate\"><span class=\"pre\">torch.float</span></code></td>\n",
    "<td><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">torch.FloatTensor</span></code></td>\n",
    "<td><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">torch.cuda.FloatTensor</span></code></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td>64-bit floating point</td>\n",
    "<td><code class=\"docutils literal notranslate\"><span class=\"pre\">torch.float64</span></code> or <code class=\"docutils literal notranslate\"><span class=\"pre\">torch.double</span></code></td>\n",
    "<td><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">torch.DoubleTensor</span></code></td>\n",
    "<td><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">torch.cuda.DoubleTensor</span></code></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td>16-bit floating point</td>\n",
    "<td><code class=\"docutils literal notranslate\"><span class=\"pre\">torch.float16</span></code> or <code class=\"docutils literal notranslate\"><span class=\"pre\">torch.half</span></code></td>\n",
    "<td><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">torch.HalfTensor</span></code></td>\n",
    "<td><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">torch.cuda.HalfTensor</span></code></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td>8-bit integer (unsigned)</td>\n",
    "<td><code class=\"docutils literal notranslate\"><span class=\"pre\">torch.uint8</span></code></td>\n",
    "<td><a class=\"reference internal\" href=\"#torch.ByteTensor\" title=\"torch.ByteTensor\"><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">torch.ByteTensor</span></code></a></td>\n",
    "<td><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">torch.cuda.ByteTensor</span></code></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td>8-bit integer (signed)</td>\n",
    "<td><code class=\"docutils literal notranslate\"><span class=\"pre\">torch.int8</span></code></td>\n",
    "<td><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">torch.CharTensor</span></code></td>\n",
    "<td><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">torch.cuda.CharTensor</span></code></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td>16-bit integer (signed)</td>\n",
    "<td><code class=\"docutils literal notranslate\"><span class=\"pre\">torch.int16</span></code> or <code class=\"docutils literal notranslate\"><span class=\"pre\">torch.short</span></code></td>\n",
    "<td><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">torch.ShortTensor</span></code></td>\n",
    "<td><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">torch.cuda.ShortTensor</span></code></td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td>32-bit integer (signed)</td>\n",
    "<td><code class=\"docutils literal notranslate\"><span class=\"pre\">torch.int32</span></code> or <code class=\"docutils literal notranslate\"><span class=\"pre\">torch.int</span></code></td>\n",
    "<td><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">torch.IntTensor</span></code></td>\n",
    "<td><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">torch.cuda.IntTensor</span></code></td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td>64-bit integer (signed)</td>\n",
    "<td><code class=\"docutils literal notranslate\"><span class=\"pre\">torch.int64</span></code> or <code class=\"docutils literal notranslate\"><span class=\"pre\">torch.long</span></code></td>\n",
    "<td><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">torch.LongTensor</span></code></td>\n",
    "<td><code class=\"xref py py-class docutils literal notranslate\"><span class=\"pre\">torch.cuda.LongTensor</span></code></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"tensor A\\n\")\n",
    "A = torch.Tensor([[1., 1.], [1., 2.]])\n",
    "print(\"type = \", A.type())\n",
    "print(\"shape = \", A.shape)\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\ntensor B\\n\")\n",
    "B = torch.Tensor([[2., 2.], [3., 4.]])\n",
    "print(\"type = \", B.type())\n",
    "print(\"shape = \", B.shape)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\ntensor C\\n\")\n",
    "C = torch.ones((4,2,2))\n",
    "print(\"type = \", C.type())\n",
    "print(\"shape = \", C.shape)\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\ntensor D\\n\")\n",
    "D = torch.rand((3,3))\n",
    "print(\"type = \", D.type())\n",
    "print(\"shape = \", D.shape)\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Como em NumPy, podemos alterar os tensores.\n",
    "\n",
    "Fazemos o uso de certos métodos para isso:\n",
    "\n",
    "- Usamos`.view()` para alterar a **forma** do tensor.\n",
    "\n",
    "- Usamos `.type()` para alterar o **tipo** do tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_flat = A.view((4,1))\n",
    "A_flat = A_flat.type(torch.ShortTensor)\n",
    "print(\"tensor A_flat\\n\")\n",
    "print(\"type = \", A_flat.type())\n",
    "print(\"shape = \", A_flat.shape)\n",
    "print(A_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_flat = B.view((4,1))\n",
    "B_flat = B_flat.type(torch.ByteTensor)\n",
    "print(\"\\ntensor B_flat\\n\")\n",
    "print(\"type = \", B_flat.type())\n",
    "print(\"shape = \", B_flat.shape)\n",
    "print(B_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fazemos o slicing de modo usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"A[0][0] = \",A[0][0])\n",
    "print(\"A[0:1,:] = \",A[0:1,:])\n",
    "print(\"A[1,:] = \",A[1,:])\n",
    "print(\"B_flat[-1] = \",B_flat[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operações\n",
    "Além de criar tensores podemos aplicar uma série de operações sobre eles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"A = \",A)\n",
    "print()\n",
    "print(\"B = \",B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"- (soma)\\n A + B = \\n\", A + B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"- (multiplicação por escalar)\\n A * 9.2 = \\n\", A * 9.2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"- (hadammar product -- multiplicação elemento a elemento)\\n A * B = \\n\", A * B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"- (multiplicação de matrix)\\n torch.matmul(A, B) = \\n\",torch.matmul(A, B)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"- (redução por soma)\\n torch.sum(A) = \\n\", torch.sum(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"- (redução por média)\\n torch.mean(B) = \\n\", torch.mean(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "print(\"- (aplicando uma função escalar num tensor)\\n sigmoid(A) = \\n\", sigmoid(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversão para Numpy\n",
    "\n",
    "Ainda vamos trabalhar com os ndarrays do NumPy.\n",
    "Fazer a transição entre NumPy e PyTorch é bem simples. Podemos inicializar um tensor passando um array, ou podemos usar a função `torch.from_numpy()`.\n",
    "\n",
    "Note que quando inicializamos um tensor com `torch.Tensor()` estamos inicializando um tensor de tipo `torch.FloatTensor`. Assim, para evitar mudança de tipo é preferível usar `torch.from_numpy()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_array = np.array([[2.3, 4.5, 2.3], [1.3, 2.5, 5.3]])\n",
    "print(\"my_array = \\n{}\".format(my_array))\n",
    "print(\"\\nmy_array.dtype =\", my_array.dtype)\n",
    "print(\"\\nmy_array.shape =\", my_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n(NumPy array -> Pytorch tensor) usando torch.Tensor()\\n\")\n",
    "my_tensor = torch.Tensor(my_array)\n",
    "print(\"my_tensor = \\n{}\".format(my_tensor))\n",
    "print(\"\\nmy_tensor.type =\", my_tensor.type())\n",
    "print(\"\\nmy_tensor.shape =\", my_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n(NumPy array -> Pytorch tensor) usando torch.from_numpy()\\n\")\n",
    "my_other_tensor = torch.from_numpy(my_array)\n",
    "print(\"my_other_tensor = \\n{}\".format(my_other_tensor))\n",
    "print(\"\\nmy_other_tensor.type =\", my_other_tensor.type())\n",
    "print(\"\\nmy_other_tensor.shape =\", my_other_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n(Pytorch tensor -> NumPy array) usando .numpy()\\n\")\n",
    "my_other_array = my_other_tensor.numpy()\n",
    "print(\"my_other_array = \\n{}\".format(my_other_array))\n",
    "print(\"\\nmy_other_array.type =\", my_other_array.dtype)\n",
    "print(\"\\nmy_other_array.shape =\", my_other_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grafo de computação\n",
    "\n",
    "Como outras bibliotecas de deep learning, PyTorch faz uso da idéia de **grafo de computação** para implementar o algoritmo de back propagation.\n",
    "Diferentemente de bibliotecas como [TensorFlow](https://www.tensorflow.org/), PyTorch trabalha com **grafos dinâmicos** (em contrapartida, TensorFlow trabalha com **grafos estáticos**). Em implementações que fazem uso de grafos estáticos precisamos definir primeiro o grafo de computação e depois vamos injetando os dados no grafo. Uma vez feito isso, podemos sempre acessar cada nó no grafo e o gradiente da folha com respeito aos parâmetros. Num grafo dinâmico, por sua vez, depois de fazermos o *forward* e o *backward pass* o grafo é apagado para liberar memória.\n",
    "\n",
    "Se você não está familiarizado com essas noções, volte nos slides do curso:\n",
    "\n",
    "- [grafo de computação (caso escalar)](https://github.com/MLIME/MAC0460/blob/master/slides/backprop1/pdf/BackpropLecture1.pdf)\n",
    "- [cálculo vetorial](https://github.com/MLIME/MAC0460/blob/master/slides/backprop2/pdf/BackpropLecture2.pdf)   \n",
    "\n",
    "#### Exemplo\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td>\n",
    "<img align=\"middle\"  width='300' heith='100'   src='images/simple_example.png'>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Esse exemplo simples de grafo de computaçao tem também uma implementação simples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.Tensor([[1., 1.], [1., 2.]])\n",
    "B = torch.Tensor([[2., 2.], [3., 4.]])\n",
    "C = A * B\n",
    "u = torch.sum(C)\n",
    "\n",
    "print(\"A = \",A)\n",
    "print()\n",
    "print(\"B = \",B)\n",
    "print()\n",
    "print(\"C = \",C)\n",
    "print()\n",
    "print(\"u = \",u)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diferenciação automática\n",
    "\n",
    "- Usamos o atributo `requires_grad` (`False` por padrão) para indicar quais tensores são treináveis. No grafo de computação se `z` é uma tensor com `requires_grad=True` e `z` é pai de `b`, então `b` é uma tensor com `requires_grad=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(5, 5)\n",
    "y = torch.randn(5, 5)\n",
    "z = torch.randn((5, 5), requires_grad=True)\n",
    "a = x + y\n",
    "b = a + z\n",
    "print(\"a.requires_grad =\", a.requires_grad)\n",
    "print(\"b.requires_grad =\", b.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dado uma variável folha `u`, usamos o método `u.backward()` para performar o back propagation e assim obter os gradientes de u com respeito às variáveis de interesse. Esse método vai funcionar apenas quando `u` for um tensor escalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    b.backward()\n",
    "    print(z.grad)    \n",
    "except RuntimeError as e:\n",
    "    print(e)\n",
    "\n",
    "u = torch.sum(b)\n",
    "u.backward()\n",
    "print(\"\\nz.grad =\\n\",z.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Podemos alterar o atributo `requires_grad` depois de inicializar o tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.Tensor([[1., 1.], [1., 2.]])\n",
    "B = torch.Tensor([[2., 2.], [3., 4.]])\n",
    "A.requires_grad = True\n",
    "C = A * B\n",
    "d = torch.sum(C)\n",
    "d.backward()\n",
    "print(\"A.grad =\\n\", A.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Por ser um grafo dinâmico, temos que o grafo é liberado apos uma execução. Se tentarmos calcular o grandiente mais de uma vez vamos encontrar um erro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    A = torch.Tensor([[1., 1.], [1., 2.]])\n",
    "    B = torch.Tensor([[2., 2.], [3., 4.]])\n",
    "    A.requires_grad = True\n",
    "    C = A * B\n",
    "    d = torch.sum(C)\n",
    "    for i in range(5):\n",
    "        print(\"i={}\".format(i))\n",
    "        d.backward()\n",
    "        print(A.grad)\n",
    "        print()\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Para manter o grafo é preciso definir o parâmetro `retain_graph=True`. Note que os gradientes são somados a cada iteração."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.Tensor([[1., 1.], [1., 2.]])\n",
    "B = torch.Tensor([[2., 2.], [3., 4.]])\n",
    "A.requires_grad = True\n",
    "C = A * B\n",
    "d = torch.sum(C)\n",
    "for i in range(5):\n",
    "    print(\"i={}\".format(i))\n",
    "    d.backward(retain_graph=True)\n",
    "    print(A.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Podemos zerar o gradiente a cada iteração para que o resultado não seja acumulado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.Tensor([[1., 1.], [1., 2.]])\n",
    "B = torch.Tensor([[2., 2.], [3., 4.]])\n",
    "A.requires_grad = True\n",
    "C = A * B\n",
    "d = torch.sum(C)\n",
    "for i in range(5):\n",
    "    print(\"i={}\".format(i))\n",
    "    d.backward(retain_graph=True)\n",
    "    print(A.grad)\n",
    "    A.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercícios\n",
    "\n",
    "Diferenciação automática é muito útil, mas existe um perigo: pode parecer que todo o trabalho feito por bibliotecas de *deep learning* (como PyTorch e TensorFlow) se assemelhe a mágica. Isso não apresenta um problema a curto prazo, mas a longo prazo [na hora de criar e testar novos modelos isso se torna um problema](https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b). \n",
    "\n",
    "Por isso, nesse notebook vamos trabalhar com alguns exercícios que obrigam a lembrar de como o algoritmo de backpropagation funciona."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **Exercício 1)**\n",
    "\n",
    "Considere o grafo cujas raizes são escalares:\n",
    "\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td>\n",
    "<img align=\"middle\" width='400' heith='100' src='images/multchain.png'>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "Complete a função `graph1`.\n",
    "\n",
    "Essa função deve montar o grafo acima e devolver a saída da variável $f$ junto com duas versões da derivada parcial $\\frac{\\partial f}{\\partial a}$: uma calculada automaticamente pelo PyTorch e outra calculada por você usando o NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph1(a_np, b_np, c_np):\n",
    "    \"\"\"\n",
    "    Computes the graph\n",
    "        - x = a * c\n",
    "        - y = a + b\n",
    "        - f = x / y\n",
    "\n",
    "    Computes also df/da using\n",
    "        - Pytorchs's automatic differentiation (auto_grad)\n",
    "        - user's implementation of the gradient (user_grad)\n",
    "\n",
    "    :param a_np: input variable a\n",
    "    :type a_np: np.ndarray(shape=(1,), dtype=float64)\n",
    "    :param b_np: input variable b\n",
    "    :type b_np: np.ndarray(shape=(1,), dtype=float64)\n",
    "    :param c_np: input variable c\n",
    "    :type c_np: np.ndarray(shape=(1,), dtype=float64)\n",
    "    :return: f, auto_grad, user_grad\n",
    "    :rtype: torch.DoubleTensor(shape=[1]),\n",
    "            torch.DoubleTensor(shape=[1]),\n",
    "            numpy.float64\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE:\n",
    "    raise NotImplementedError(\"falta completar a função graph1\")\n",
    "    # END YOUR CODE\n",
    "    return f, auto_grad, user_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testes do exercício 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(1000):\n",
    "    a_np = np.random.rand(1)\n",
    "    b_np = np.random.rand(1)\n",
    "    c_np = np.random.rand(1)\n",
    "    f, auto_grad, user_grad = graph1(a_np, b_np, c_np)\n",
    "    manual_f = (a_np * c_np) / (a_np + b_np)\n",
    "    assert np.isclose(f.data.numpy()[0], manual_f, atol=1e-4), \"Valor do f com problemas\"\n",
    "    assert np.isclose(auto_grad.numpy()[0], user_grad), \"Derivada parcial com problemas\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **Exercício 2)**\n",
    "\n",
    "Considere o grafo:\n",
    "\n",
    "\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "<td>\n",
    "<img align=\"middle\"  width='300' heith='100' src='images/vector_graph.png'>\n",
    "</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "Complete a função `graph2`.\n",
    "\n",
    "Essa função deve montar o grafo acima e devolver a saída da variável $f$ junto com duas versões do gradiente $\\frac{\\partial f}{\\partial W}$: uma calculada automaticamente pelo PyTorch e outra calculada por você usando o NumPy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def graph2(W_np, x_np, b_np):\n",
    "    \"\"\"\n",
    "    Computes the graph\n",
    "        - u = Wx + b\n",
    "        - g = sigmoid(u)\n",
    "        - f = sum(g)\n",
    "\n",
    "    Computes also df/dW using\n",
    "        - pytorchs's automatic differentiation (auto_grad)\n",
    "        - user's own manual differentiation (user_grad)\n",
    "        \n",
    "    F.sigmoid may be useful here\n",
    "\n",
    "    :param W_np: input variable W\n",
    "    :type W_np: np.ndarray(shape=(d,d), dtype=float64)\n",
    "    :param x_np: input variable x\n",
    "    :type x_np: np.ndarray(shape=(d,1), dtype=float64)\n",
    "    :param b_np: input variable b\n",
    "    :type b_np: np.ndarray(shape=(d,1), dtype=float64)\n",
    "    :return: f, auto_grad, user_grad\n",
    "    :rtype: torch.DoubleTensor(shape=[1]),\n",
    "            torch.DoubleTensor(shape=[d, d]),\n",
    "            np.ndarray(shape=(d,d), dtype=float64)\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE:\n",
    "    raise NotImplementedError(\"falta completar a função graph2\")\n",
    "    # END YOUR CODE\n",
    "    return f, auto_grad, user_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testes do exercício 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 1000\n",
    "sizes = np.random.randint(2,10, size=(iterations))\n",
    "for i in range(iterations):\n",
    "    size = sizes[i]\n",
    "    W_np = np.random.rand(size, size)\n",
    "    x_np = np.random.rand(size, 1)\n",
    "    b_np = np.random.rand(size, 1)\n",
    "    f, auto_grad, user_grad = graph2(W_np, x_np, b_np)\n",
    "    manual_f = np.sum(sigmoid(np.matmul(W_np, x_np) + b_np))\n",
    "    assert np.isclose(f.data.numpy(), manual_f, atol=1e-4), \"Valor do f com problemas\"\n",
    "    assert np.allclose(auto_grad.numpy(), user_grad), \"Gradiente com problemas\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicando uma variação do algoritimo SGD no exemplo de regressão linear\n",
    "\n",
    "\n",
    "[Momentum](https://distill.pub/2017/momentum/) é um método que ajuda a acelerar o algoritmo SGD. Ele funciona ao se adicionar um vetor de atualização $\\mathbf{z}$:\n",
    "\n",
    "\n",
    "**Stochastic gradient descent with momentum**\n",
    "\n",
    "- $\\mathbf{w}(0) = \\mathbf{w}$\n",
    "- $\\mathbf{z}(0) = \\mathbf{0}$\n",
    "- for $t = 0, 1, 2, \\dots$ do\n",
    "    * Sample a minibatch of $m$ examples from the training data.\n",
    "    * Compute the gradient estimate $\\hat{\\nabla_{\\mathbf{w}(t)}J(\\mathbf{w}(t))}$\n",
    "    * Calculate update vector: $\\mathbf{z}(t+1) = \\gamma \\mathbf{z}(t) + \\hat{\\nabla_{\\mathbf{w}(t)}J(\\mathbf{w}(t))}$\n",
    "    * Apply update : $\\mathbf{w}(t+1) = \\mathbf{w}(t) - \\eta \\mathbf{z}(t+1)$\n",
    "\n",
    "\n",
    "O parâmetro $\\gamma \\in \\mathbb{R}_{\\geq}$ é chamado de **momentum**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos implementar esse algoritmo usando PyTorch no dataset sintético que estavamos usando nos notebooks passados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import r_squared, randomize_in_place, get_housing_prices_data, add_feature_ones\n",
    "from plots import simple_step_plot, plot_points_regression\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def standardize(X):\n",
    "    \"\"\"\n",
    "    Returns standardized version of the ndarray 'X'.\n",
    "\n",
    "    :param X: input array\n",
    "    :type X: np.ndarray(shape=(N, d))\n",
    "    :return: standardized array\n",
    "    :rtype: np.ndarray(shape=(N, d))\n",
    "    \"\"\"\n",
    "\n",
    "    X_T = X.T\n",
    "    all_mean = []\n",
    "    all_std = []\n",
    "    for i in range(X_T.shape[0]):\n",
    "        all_mean.append(np.mean(X_T[i]))\n",
    "        all_std.append(np.std(X_T[i]))\n",
    "\n",
    "    X_out = (X - all_mean) / all_std\n",
    "\n",
    "    return X_out\n",
    "\n",
    "\n",
    "X, y = get_housing_prices_data(N=350, verbose=False)\n",
    "randomize_in_place(X, y)\n",
    "train_X = X[0:250]\n",
    "train_y = y[0:250]\n",
    "valid_X = X[250:300]\n",
    "valid_y = y[250:300]\n",
    "test_X = X[300:]\n",
    "test_y = y[300:]\n",
    "train_X_norm = standardize(train_X)\n",
    "train_y_norm = standardize(train_y)\n",
    "train_y_norm = train_y_norm.astype(\"float64\") \n",
    "train_X_1 = add_feature_ones(train_X_norm)\n",
    "test_X_norm = standardize(test_X)\n",
    "test_y_norm = standardize(test_y)\n",
    "test_X_1 = add_feature_ones(test_X_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Exercício 3)** \n",
    "\n",
    "Implemente o algoritmo *stochastic gradient descent with momentum* em PyTorch. A função abaixo deve retornar três coisas: o vetor de pesos $\\mathbf{w}$, uma lista com cada peso obtido ao longo do treinamento, e uma lista com o custo de cada peso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD_with_momentum(X, y, inital_w, iterations, batch_size, learning_rate, momentum):\n",
    "    \"\"\"\n",
    "    Performs batch gradient descent optimization using momentum.\n",
    "\n",
    "    :param X: design matrix\n",
    "    :type X: np.ndarray(shape=(N, d))\n",
    "    :param y: regression targets\n",
    "    :type y: np.ndarray(shape=(N, 1))\n",
    "    :param inital_w: initial weights\n",
    "    :type inital_w: np.array(shape=(d, 1))\n",
    "    :param iterations: number of iterations\n",
    "    :type iterations: int\n",
    "    :param batch_size: size of the minibatch\n",
    "    :type batch_size: int\n",
    "    :param learning_rate: learning rate\n",
    "    :type learning_rate: float\n",
    "    :param momentum: accelerate parameter\n",
    "    :type momentum: float\n",
    "    :return: weights, weights history, cost history\n",
    "    :rtype: np.array(shape=(d, 1)), list, list\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE:\n",
    "    raise NotImplementedError(\"falta completar a função SGD_with_momentum\")\n",
    "    # END YOUR CODE    \n",
    "\n",
    "    return w_np, weights_history, cost_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testes para o exercício 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "init = time.time()\n",
    "w, weights_history, cost_history = SGD_with_momentum(X=train_X_1,\n",
    "                                                     y=train_y_norm,\n",
    "                                                     inital_w=np.array([[9.2], [-30.3]]),\n",
    "                                                     iterations=1000,\n",
    "                                                     batch_size=32,\n",
    "                                                     learning_rate=0.01,\n",
    "                                                     momentum=0.01)\n",
    "assert cost_history[-1] < cost_history[0]\n",
    "assert type(w) == np.ndarray\n",
    "assert len(weights_history) == len(cost_history)\n",
    "init = time.time() - init\n",
    "print(\"Tempo de treinamento = {:.8f}(s)\".format(init))\n",
    "print(\"Tem que ser em menos de 1.2 segundos\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Podemos ver como o parâmetro momentum interfere na otimização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_params = [(300, 0.01, 0.01),\n",
    "                (300, 0.01, 0.1),\n",
    "                (300, 0.01, 0.3),\n",
    "                (300, 0.01, 0.5),\n",
    "                (300, 0.01, 0.8)]\n",
    "               \n",
    "\n",
    "for params in hyper_params:\n",
    "    iterations = params[0]\n",
    "    learning_rate = params[1]\n",
    "    momentum = params[2] \n",
    "    best_w, weights_history, cost_history = SGD_with_momentum(X=train_X_1,\n",
    "                                                y=train_y_norm,\n",
    "                                                inital_w=np.array([[9.2], [-30.3]]),\n",
    "                                                iterations=iterations,\n",
    "                                                batch_size=1,\n",
    "                                                learning_rate=learning_rate,\n",
    "                                                momentum=momentum)\n",
    "    simple_step_plot([cost_history],\n",
    "                     \"loss\",\n",
    "                     'Training loss\\nlearning rate = {} | iterations = {} | momentum = {}'.format(learning_rate,\n",
    "                                                                              iterations, momentum),\n",
    "                     figsize=(8, 8))\n",
    "\n",
    "prediction = test_X_1.dot(best_w)\n",
    "prediction = (prediction * np.std(train_y)) + np.mean(train_y)\n",
    "r_2 = r_squared(test_y, prediction)\n",
    "\n",
    "plot_points_regression(test_X,\n",
    "                       test_y,\n",
    "                       title='Prediction on test data',\n",
    "                       xlabel=\"m\\u00b2\",\n",
    "                       ylabel='$',\n",
    "                       prediction=prediction,\n",
    "                       r_squared=r_2,\n",
    "                       legend=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
